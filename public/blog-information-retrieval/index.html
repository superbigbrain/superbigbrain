<!DOCTYPE html>
<html lang="en-us">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>An Overview of Information Retrieval | Bandit</title>
<meta name="title" content="An Overview of Information Retrieval" />
<meta name="description" content="Key Ideas and Themes Analysis Making text searchable. Lemmatization, stemming, stop words, and tokenization.
Indexing Mapping terms to documents. Inverted index. Dictionaries and postings lists. Index compression and updates.
Retrieval Finding relevant documents. Querying and ranking. Boolean and vector space models.
Evaluation Measuring success. Effectiveness and efficiency. Precision and recall.
Questions  What are the key ideas in this field? What are the common themes in this field? What are the open problems in this field?" />
<meta name="keywords" content="" />


<meta property="og:title" content="An Overview of Information Retrieval" />
<meta property="og:description" content="Key Ideas and Themes Analysis Making text searchable. Lemmatization, stemming, stop words, and tokenization.
Indexing Mapping terms to documents. Inverted index. Dictionaries and postings lists. Index compression and updates.
Retrieval Finding relevant documents. Querying and ranking. Boolean and vector space models.
Evaluation Measuring success. Effectiveness and efficiency. Precision and recall.
Questions  What are the key ideas in this field? What are the common themes in this field? What are the open problems in this field?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/blog-information-retrieval/" />
<meta property="article:published_time" content="2020-10-08T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-10-08T00:00:00+00:00" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="An Overview of Information Retrieval"/>
<meta name="twitter:description" content="Key Ideas and Themes Analysis Making text searchable. Lemmatization, stemming, stop words, and tokenization.
Indexing Mapping terms to documents. Inverted index. Dictionaries and postings lists. Index compression and updates.
Retrieval Finding relevant documents. Querying and ranking. Boolean and vector space models.
Evaluation Measuring success. Effectiveness and efficiency. Precision and recall.
Questions  What are the key ideas in this field? What are the common themes in this field? What are the open problems in this field?"/>



<meta itemprop="name" content="An Overview of Information Retrieval">
<meta itemprop="description" content="Key Ideas and Themes Analysis Making text searchable. Lemmatization, stemming, stop words, and tokenization.
Indexing Mapping terms to documents. Inverted index. Dictionaries and postings lists. Index compression and updates.
Retrieval Finding relevant documents. Querying and ranking. Boolean and vector space models.
Evaluation Measuring success. Effectiveness and efficiency. Precision and recall.
Questions  What are the key ideas in this field? What are the common themes in this field? What are the open problems in this field?">
<meta itemprop="datePublished" content="2020-10-08T00:00:00+00:00" />
<meta itemprop="dateModified" content="2020-10-08T00:00:00+00:00" />
<meta itemprop="wordCount" content="1698">



<meta itemprop="keywords" content="" />

<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  body {
    font-family: Verdana, sans-serif;
    margin: auto;
    padding: 20px;
    max-width: 720px;
    text-align: left;
    background-color: #fff;
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: #444;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6,
  strong,
  b {
    color: #222;
  }

  a {
    color: #3273dc;
     
  }

  .title {
    text-decoration: none;
    border: 0;
  }

  .title span {
    font-weight: 400;
  }

  nav a {
    margin-right: 10px;
  }

  textarea {
    width: 100%;
    font-size: 16px;
  }

  input {
    font-size: 16px;
  }

  content {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  img {
    max-width: 100%;
  }

  code {
    padding: 2px 5px;
    position: relative;
    background-color: #fff;
    font-family: 'Source Code Pro', monospace;
  }

  pre code {
    color: #222;
    display: block;
    font-size: 14px;
    padding: 20px;
    overflow-x: scroll;
  }

  blockquote {
    background-color: #F3F4F5;
    color: #222;
    margin: 0;
    margin-bottom: 1rem;
    padding: 1rem 2rem;
    font-style: italic;
  }

  blockquote em {
    font-style: normal;
  }

  footer {
    padding: 25px;
    text-align: center;
  }

  .helptext {
    color: #777;
    font-size: small;
  }

  .errorlist {
    color: #eba613;
    font-size: small;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 130px;
  }

  ul.blog-posts li a:visited {
    color: #8b6fcb;
  }

  @media (prefers-color-scheme: dark) {
    body {
      background-color: #333;
      color: #ddd;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6,
    strong,
    b {
      color: #eee;
    }

    a {
      color: #8cc2dd;
    }

    code {
      background-color: #777;
    }

    pre code {
      color: #ddd;
    }

    blockquote {
      color: #ccc;
    }

    textarea,
    input {
      background-color: #252525;
      color: #ddd;
    }

    .helptext {
      color: #aaa;
    }
  }

  .language-bash:before {
    content: '$';
    left: 5px;
    position: absolute;
  }

  ::selection {
    background: none;
    background-color: #fef4ad;
  }
</style>

<link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro&display=swap" rel="stylesheet">
</head>

<body>
  <header><a href="/" class="title">
  <h2>Bandit</h2>
</a>
<nav><a href="/">Home</a>
<a href="/blog">Blog</a>
<a href="/guides/">Guides</a></nav>
</header>
  <main>

<h1>An Overview of Information Retrieval</h1>
<p>
  
  <i>Draft &mdash;</i>
  
  
  <i>
    
    <time datetime='2020-10-08' pubdate>
      October 8, 2020
    </time>
  </i>

  
</p>

<content>
  <h3 id="key-ideas-and-themes">Key Ideas and Themes</h3>
<h4 id="analysis">Analysis</h4>
<p>Making text searchable. Lemmatization, stemming, stop words, and tokenization.</p>
<h4 id="indexing">Indexing</h4>
<p>Mapping terms to documents. Inverted index. Dictionaries and postings lists.
Index compression and updates.</p>
<h4 id="retrieval">Retrieval</h4>
<p>Finding relevant documents. Querying and ranking. Boolean and vector space models.</p>
<h4 id="evaluation">Evaluation</h4>
<p>Measuring success. Effectiveness and efficiency. Precision and recall.</p>
<h3 id="questions">Questions</h3>
<ul>
<li>What are the key ideas in this field?</li>
<li>What are the common themes in this field?</li>
<li>What are the open problems in this field?</li>
<li>What are the important problems in this field?</li>
<li>What has been done in this field?</li>
<li>What hasn&rsquo;t been done in this field?</li>
<li>What questions related to this field would I like answers to?</li>
<li>What does a knowledge graph of this field look like?</li>
<li>What are the common terms in this field?</li>
<li>What is the difference between an &ldquo;information retrieval system&rdquo; and an
&ldquo;automated information retrieval system&rdquo;?</li>
<li>What types of information and data can be searched?</li>
<li>By what two dimensions are information retrieval models classified?</li>
<li>What are the properties of a &ldquo;set theoretic&rdquo; information retrieval model?</li>
<li>What are the properties of an &ldquo;algebraic&rdquo; information retrieval model?</li>
<li>What are the properties of a &ldquo;probablistic&rdquo; information retrieval model?</li>
<li>What are the properties of a &ldquo;feature-based&rdquo; information retrieval model?</li>
<li>How do models without term-interdependencies treat different terms?</li>
<li>How do models with immanent term interdependencies treat different terms?</li>
<li>How do models with transcendent term interdependencies treat different terms?</li>
<li>What is a vector space model?</li>
<li>What is the difference between information retrieval and natural language
processing?</li>
</ul>
<h2 id="analysis-1">Analysis</h2>
<p>&hellip;</p>
<h2 id="indexing-1">Indexing</h2>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python3" data-lang="python3"><span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">InvertedIndex</span>:
    <span style="color:#d14">&#34;&#34;&#34;
</span><span style="color:#d14">    Maps terms in a dictionary to occurences in a postings list.
</span><span style="color:#d14">
</span><span style="color:#d14">    Performance:
</span><span style="color:#d14">
</span><span style="color:#d14">        `.next(term, position)` — O(1)
</span><span style="color:#d14">        `.previous(term, position)` — O(1)
</span><span style="color:#d14">    &#34;&#34;&#34;</span>
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">__init__</span>(<span style="color:#999">self</span>, <span style="color:#000;font-weight:bold">*</span>args, <span style="color:#000;font-weight:bold">**</span>kwargs) <span style="color:#000;font-weight:bold">-&gt;</span> <span style="color:#000;font-weight:bold">None</span>:
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#000;font-weight:bold">None</span>

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">get</span>(<span style="color:#999">self</span>, term: <span style="color:#0086b3">str</span> <span style="color:#000;font-weight:bold">=</span> <span style="color:#000;font-weight:bold">None</span>) <span style="color:#000;font-weight:bold">-&gt;</span> Occurence:
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#000;font-weight:bold">None</span>

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">_merge</span>(<span style="color:#999">self</span>, other: InvertedIndex) <span style="color:#000;font-weight:bold">-&gt;</span> InvertedIndex:
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#999">self</span> <span style="color:#000;font-weight:bold">+</span> other
</code></pre></div><h2 id="retrieval-1">Retrieval</h2>
<p>&hellip;</p>
<h2 id="evaluation-1">Evaluation</h2>
<p>&hellip;</p>
<h4 id="thoughts">Thoughts</h4>
<ul>
<li>Are information retrieval and natural language processing inherently deeply
connected? It seems like a lot of the work required to make a good IR system
is actually NLP-related work of making some text searchable.</li>
</ul>
<h4 id="sources">Sources</h4>
<h5 id="evaluation-measureshttpsenwikipediaorgwikievaluation_measures_information_retrieval"><a href="https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)">Evaluation Measures</a></h5>
<h5 id="relevancehttpsenwikipediaorgwikirelevance_information_retrieval"><a href="https://en.wikipedia.org/wiki/Relevance_(information_retrieval)">Relevance</a></h5>
<h5 id="tf-idfhttpsenwikipediaorgwikitfe28093idf"><a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">tf-idf</a></h5>
<h5 id="full-text-searchhttpsenwikipediaorgwikifull-text_search"><a href="https://en.wikipedia.org/wiki/Full-text_search">Full-text Search</a></h5>
<h5 id="information-retrievalhttpsenwikipediaorgwikiinformation_retrieval"><a href="https://en.wikipedia.org/wiki/Information_retrieval">Information Retrieval</a></h5>
<ul>
<li>Information retrieval is about finding needles in haystacks.</li>
<li>&ldquo;Information retrieval is the science of searching for information in a
document, searching for documents themselves, and also searching for the
metadata that describes data, and for databases of texts, images, or sounds.&rdquo;</li>
<li>Search engines like Google are by-far the most well-known examples of
information retrieval systems.</li>
<li>&ldquo;Most information retrieval systems compute a numeric score on how well each
object in the database matches the query, and rank the objects according to
this value.&rdquo;</li>
<li>Set-theoretic models represent documents as sets of words or phrases.
<ul>
<li>E.g. Standard Boolean model, Extended Boolean model, Fuzzy retrieval</li>
</ul>
</li>
<li>Algebraric models represent documents as vectors, matrices, or tuples.
<ul>
<li>E.g. Vector space model, Latent Semantic Indexing</li>
</ul>
</li>
<li>Probablistic models compute probabilities that a document is relevant to a
given query.
<ul>
<li>E.g. Binary independence model, Probablistic relevance model</li>
</ul>
</li>
<li>Feature-based models seek to combine vectors of features into a single
relevance score.
<ul>
<li>E.g. Learning to rank</li>
</ul>
</li>
</ul>
<h5 id="papers-we-loveinformation-retrievalhttpsgithubcompapers-we-lovepapers-we-lovetreemasterinformation_retrieval"><a href="https://github.com/papers-we-love/papers-we-love/tree/master/information_retrieval">papers-we-love/information-retrieval</a></h5>
<h5 id="an-introduction-to-information-retrieval">An Introduction to Information Retrieval</h5>
<ul>
<li>Key design principle for book: Cover what [the authors] believe to be
important in a one-semester graduate course on information retrieval.</li>
<li>Chapters 1-8 are devoted to &ldquo;the basics&rdquo; and are &ldquo;core to any course on
information retrieval&rdquo;.
<ul>
<li>NOTE: Chapters 1-5 deal only with boolean queries. Either a document matches
the query or it doesn&rsquo;t.</li>
<li>Chapter 1: Inverted indices, boolean queries</li>
<li>Chapter 2: Pre-index document processing, inverted index augmentation</li>
<li>Chapter 3: Dictionaries, spelling correction</li>
<li>Chapter 4: Large-scale inverted index construction</li>
<li>Chapter 5: Inverted index and dictionary compression</li>
<li>Chapter 6: Scoring and Term weighting</li>
<li>Chapter 7: Score computation</li>
<li>Chapter 8: Evaluation</li>
</ul>
</li>
</ul>
<h5 id="information-retrieval-implementing-and-evaluating-search-engines">Information Retrieval: Implementing and Evaluating Search Engines</h5>
<ul>
<li>Introduction
<ul>
<li>Chapter 1: Introduction</li>
<li>Chapter 2: Overview of indexing, ranking and retrieval, and evaluation</li>
<li>Chapter 3: Overview of tokenization</li>
</ul>
</li>
<li>Indexing
<ul>
<li>Chapter 4: Static inverted indices</li>
<li>Chapter 5: Querying inverted indices</li>
<li>Chapter 6: Inverted index compression</li>
<li>Chapter 7: Dynamic inverted indices</li>
</ul>
</li>
<li>Retrieval and Ranking
<ul>
<li>Chapter 8: Probabilistic retrieval</li>
<li>Chapter 9: Language modeling</li>
<li>Chapter 10: Categorization and filtering</li>
<li>Chapter 11: Metalearning</li>
</ul>
</li>
<li>Evaluation
<ul>
<li>Chapter 12: Effectiveness</li>
<li>Chapter 13: Efficiency</li>
</ul>
</li>
</ul>
<h6 id="1-introduction">1. Introduction</h6>
<ul>
<li>Information retrieval applications
<ul>
<li>Document routing
<ul>
<li>E.g. Google News alerts</li>
</ul>
</li>
<li>Clustering and categorization</li>
<li>Summarization</li>
<li>Information extraction</li>
<li>Topic detection and tracking</li>
<li>Expert search
<ul>
<li>Identify the expert humans in a field</li>
</ul>
</li>
<li>Question answering</li>
<li>Multimedia information retrieval
E.g. images, video, music, and speech</li>
</ul>
</li>
<li>Terminology
<ul>
<li>Information need / Topic</li>
<li>Query</li>
<li>Terms</li>
<li>Filters</li>
<li>Search engine</li>
<li>Inverted index</li>
<li>Document collection
<ul>
<li>-&gt; Collection of &ldquo;self-contained search results&rdquo;</li>
</ul>
</li>
<li>Score / retrieval status value</li>
</ul>
</li>
<li>Performance evaluation
<ul>
<li>Efficiency
<ul>
<li>Measured in units of time and space
<ul>
<li>E.g. queries/second, bytes/document</li>
<li>E.g. Response time, throughput</li>
</ul>
</li>
</ul>
</li>
<li>Effectiveness
<ul>
<li>Depends on human judgement</li>
</ul>
</li>
</ul>
</li>
<li>Term distributions</li>
<li>Language modeling</li>
<li>Markov models
<ul>
<li>Approach to representing term distributions</li>
</ul>
</li>
</ul>
<p>Questions:</p>
<ul>
<li>What is a document in an information retrieval system?</li>
<li>What are the two principal measurements of IR system performance?</li>
<li>What is a language model?</li>
<li>When is a language model useful?</li>
<li>How are language models used to achieve index compression?</li>
<li>How are language models used to detect e-mail spam?</li>
</ul>
<h6 id="2-basic-techniques">2. Basic Techniques</h6>
<ul>
<li>Inverted indices
<ul>
<li>Provides a mapping between terms and their locations of occurence in a text
collection C.</li>
<li>Each term in a <code>dictionary</code> (hash map) points to a <code>postings list</code> of the
positions in which the term appears.</li>
<li>Abstract Data Type with four methods:
<ul>
<li>first(t) returns the first position at which the term t occurs in the
collection</li>
<li>last(t) returns the last position at which the term t occurs in the
collection</li>
<li>next(t, current) returns the position of t&rsquo;s first occurence after the
<code>current</code> position</li>
<li>prev(t, current) returns the position of t&rsquo;s last occurence before the
<code>current</code> position</li>
</ul>
</li>
<li>Made up of documents
<ul>
<li>Results in postings list include a document ID and a position</li>
</ul>
</li>
<li>Document-oriented statistics:
<ul>
<li>N_t &ndash; document frequency &ndash; the number of documents in the collection
containing the term t</li>
<li>f_t,d &ndash; term frequency &ndash; the number of times the term t appears in the
document d</li>
<li>l_d &ndash; document length &ndash; measured in tokens</li>
<li>l_avg &ndash; average length &ndash; average document length across the collection</li>
<li>N &ndash; document count &ndash; total number of doucments in the collection</li>
</ul>
</li>
<li>Additional document-oriented methods for index ADT:
<ul>
<li>first_doc(t) returns the docid of the first document containing the term t</li>
<li>last_doc(t) returns the docid of the last document containing the term t</li>
<li>next_doc(t, current) returns the docid of the first document after
<code>current</code> that contains the term t</li>
<li>prev_doc(t, current) returns the docid of the last document before
<code>current</code> that contains the term t</li>
</ul>
</li>
<li>Types of inverted index:
<ul>
<li><code>docid index</code> &ndash; Contains the document identifiers for all documents in
which a term appears. Simple, but supports filtering.</li>
<li><code>frequency index</code> &ndash; Each entry in inverted list contains <code>(docid, f_t,d)</code>
&ndash; document id and term frequency value</li>
<li>Positional index?</li>
<li>Schema-independent index?</li>
</ul>
</li>
</ul>
</li>
<li>Retrieval and ranking
<ul>
<li>Queries for ranked retrieval are often expressed as term vectors &ndash; terms
seperated by whitespace</li>
<li>Boolean predicates are interpreted as filters &ndash; if a result doesn&rsquo;t match
the predicate it isn&rsquo;t included in the results set.</li>
<li>Term vectors are a summarization of the information need. Not all terms in
the vector must appear in every result.</li>
<li>Two-step retrieval:
<ul>
<li>Boolean predicates applied for filtering</li>
<li>Results ranked with respect to the query or something else</li>
</ul>
</li>
<li>Ranking attributes:
<ul>
<li>Term frequency &ndash; How often do these terms appear in a document?</li>
<li>Term proximity &ndash; How close are these terms to eachother?</li>
</ul>
</li>
<li>Vector space model
<ul>
<li>&ldquo;One of the oldest and best known of the information retrieval models&rdquo;</li>
<li>In recent years, largely overshadowed by probablistic models, language
models, and machine learning</li>
<li>&ldquo;Queries as well as documents are represented as vectors in a
high-deminensional space in which each vector component corresponds to a
term in the vocabulary of the collection.&rdquo;
<ul>
<li>Queries and documents are represented as large, space vectors.</li>
<li>Each element in the vector corresponds to a term in the collection.</li>
</ul>
</li>
<li>Documents are ranked by computing a similarity measure between the the
query vector and each document vector.
<ul>
<li>The similarity measure computes the angle between the vectors.</li>
<li>The smaller the angle, the more similar the vectors.</li>
<li>Linear algebra! Dot product!</li>
</ul>
</li>
<li>== &ldquo;Bag of Words&rdquo;</li>
</ul>
</li>
<li>TF-IDF
<ul>
<li>TF=function of term frequency</li>
<li>IDF=function of inverse document frequency</li>
<li>Used as weightings in a vector space model?</li>
</ul>
</li>
</ul>
</li>
<li>Evaluation
<ul>
<li>Recall &ndash; The fraction of relevant documents that appears in the result set</li>
<li>Precision &ndash; The fraction of the result set that is relevant</li>
<li>Sometimes combined into a single value known as an F-measure</li>
</ul>
</li>
</ul>
<h6 id="4-static-inverted-indices">4. Static Inverted Indices</h6>
<ul>
<li>Hybrid data structures:
<ul>
<li>Some data stored in memory</li>
<li>Majority stored on disk</li>
<li>Because memory is expensive and disk is cheap</li>
</ul>
</li>
<li>Principal components:
<ul>
<li>Dictionary (terms)</li>
<li>Postings Lists (occurences)</li>
</ul>
</li>
<li>Two distinct phases:
<ul>
<li>Index construction / index time</li>
<li>Query processing / query time</li>
</ul>
</li>
<li>Dictionary
<ul>
<li>Operations:
<ul>
<li>Insert a new entry for term <code>T</code></li>
<li>Find and return the entry for term <code>T</code> (if present)</li>
<li>Find and return the entries for all terms that start with a given prefix
<code>P</code></li>
</ul>
</li>
<li>Hash-based dictionaries support efficient term queries</li>
<li>Sort-based dictionaries support efficient prefix queries</li>
</ul>
</li>
<li>Postings lists
<ul>
<li>Usually account for the bulk of the data</li>
<li>Usually too big to store in main memory</li>
<li>Read from disk as-needed</li>
<li>Should be stored contiguously on disk</li>
<li>Per-term indices make on-disk lookups more efficient</li>
</ul>
</li>
<li>Index construction
<ul>
<li>In-memory vs disk-based index construction</li>
<li>Merge-based index construction
<ul>
<li>Dynamically partitions data based on available memory</li>
<li>Each partition is built into an index in-memory and flushed to disk
<ul>
<li>These are called index partitions</li>
</ul>
</li>
<li>The index partitions are merged into a single, final index</li>
</ul>
</li>
</ul>
</li>
</ul>
<h6 id="6-index-compression">6. Index Compression</h6>
<ul>
<li>&hellip;</li>
</ul>
<h6 id="7-dynamic-inverted-indices">7. Dynamic Inverted Indices</h6>
<ul>
<li>Three types of operations:
<ul>
<li>Insertions</li>
<li>Deletions</li>
<li>Updates (i.e. modifications)</li>
</ul>
</li>
<li>Inverted indices can be merged
<ul>
<li>Building a new inverted index from a list of operations and then merging it
with the existing inverted index is efficient if the merge is efficient</li>
</ul>
</li>
<li>Logarithmic Merge
<ul>
<li>Standard index maintenance policy in Lucene at the time of writing
<ul>
<li>Is this still true?</li>
</ul>
</li>
</ul>
</li>
<li>Deletions are tracked using an &ldquo;invalidation list&rdquo; because actually deleting
documents in real-time would be inefficient.</li>
<li>The process of actually deleting documents previously added to the
&ldquo;invalidation list&rdquo; is known as &ldquo;garbage collection&rdquo;.</li>
<li>Index maintenance strategies</li>
<li>Index compression</li>
<li>Garbage collection</li>
</ul>
<h5 id="search-engines-information-retrieval-in-practice">Search Engines: Information Retrieval in Practice</h5>
<ul>
<li>Chapter 4: Processing text</li>
<li>Chapter 5: Inverted indices</li>
<li>Chapter 6: Query transformations, expansion</li>
<li>Chapter 7: Retrieval</li>
<li>Chapter 8: Evaluation</li>
<li>Chapter 9: Classification, clustering</li>
<li>Chapter 11: &ldquo;Beyond Bag of Words&rdquo;</li>
</ul>

</content>
<p>
  
</p>

  </main>
  <footer>
</footer>

    
</body>

</html>
